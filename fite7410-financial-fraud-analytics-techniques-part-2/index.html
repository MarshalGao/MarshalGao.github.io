<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Marshal ">
<meta name="description" content="Clustering Basics about clustering
 aim  group a set of observations into groups (or clusters)  feature  the homogeneity (similarity) within the cluster is maximized the heterogeneity (dissimilarity) between the cluster is maximized unsupervised classification (no labels) no predefined target class number of clusters unknown meaning of clusters unknown  distance metrics
 measure the similarity and dissimilarity between observations in a group types
 metrics for continuous variables" />
<meta name="keywords" content=", fite7410, financial fraud analytics, fintech, techniques" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://www.marshalgao.com/fite7410-financial-fraud-analytics-techniques-part-2/" />


    <title>
        
            FITE7410 Financial Fraud Analytics Techniques (Part 2) :: This is Marshal 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://www.marshalgao.com/main.1abfe30dec5ae0f5c4f84acd6413b12f63e4a6f9e1e7167f42de12192bfd25e9.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://www.marshalgao.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://www.marshalgao.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://www.marshalgao.com/favicon-16x16.png">
    <link rel="manifest" href="https://www.marshalgao.com/site.webmanifest">
    <link rel="mask-icon" href="https://www.marshalgao.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://www.marshalgao.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">



<meta itemprop="name" content="FITE7410 Financial Fraud Analytics Techniques (Part 2)">
<meta itemprop="description" content="Clustering Basics about clustering
 aim  group a set of observations into groups (or clusters)  feature  the homogeneity (similarity) within the cluster is maximized the heterogeneity (dissimilarity) between the cluster is maximized unsupervised classification (no labels) no predefined target class number of clusters unknown meaning of clusters unknown  distance metrics
 measure the similarity and dissimilarity between observations in a group types
 metrics for continuous variables">
<meta itemprop="datePublished" content="2021-03-06T11:46:59+08:00" />
<meta itemprop="dateModified" content="2021-03-06T11:46:59+08:00" />
<meta itemprop="wordCount" content="2352">
<meta itemprop="image" content="https://www.marshalgao.com/"/>



<meta itemprop="keywords" content="fite7410,financial fraud analytics,fintech,techniques," />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://www.marshalgao.com/"/>

<meta name="twitter:title" content="FITE7410 Financial Fraud Analytics Techniques (Part 2)"/>
<meta name="twitter:description" content="Clustering Basics about clustering
 aim  group a set of observations into groups (or clusters)  feature  the homogeneity (similarity) within the cluster is maximized the heterogeneity (dissimilarity) between the cluster is maximized unsupervised classification (no labels) no predefined target class number of clusters unknown meaning of clusters unknown  distance metrics
 measure the similarity and dissimilarity between observations in a group types
 metrics for continuous variables"/>



    <meta property="og:title" content="FITE7410 Financial Fraud Analytics Techniques (Part 2)" />
<meta property="og:description" content="Clustering Basics about clustering
 aim  group a set of observations into groups (or clusters)  feature  the homogeneity (similarity) within the cluster is maximized the heterogeneity (dissimilarity) between the cluster is maximized unsupervised classification (no labels) no predefined target class number of clusters unknown meaning of clusters unknown  distance metrics
 measure the similarity and dissimilarity between observations in a group types
 metrics for continuous variables" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.marshalgao.com/fite7410-financial-fraud-analytics-techniques-part-2/" />
<meta property="og:image" content="https://www.marshalgao.com/"/>
<meta property="article:published_time" content="2021-03-06T11:46:59+08:00" />
<meta property="article:modified_time" content="2021-03-06T11:46:59+08:00" />




    <meta property="article:section" content="HKU" />



    <meta property="article:published_time" content="2021-03-06 11:46:59 &#43;0800 CST" />









    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://www.marshalgao.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://www.marshalgao.com/categories/technology">TECH</a></li><li><a href="https://www.marshalgao.com/categories/product-manager">PM</a></li><li><a href="https://www.marshalgao.com/categories/life">LIFE</a></li><li><a href="https://www.marshalgao.com/about/">ABOUT</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>

            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        12 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://www.marshalgao.com/fite7410-financial-fraud-analytics-techniques-part-2/">FITE7410 Financial Fraud Analytics Techniques (Part 2)</a>
      </h1>

      

      <div class="post-content">
        <p><figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/Financial%20Fraud%20Analytics%20Techniques%20(Part%202).png" alt=""></figure></p>

<h2 id="clustering">Clustering</h2>

<p><strong>Basics about clustering</strong></p>

<ul>
<li>aim

<ul>
<li>group a set of observations into groups (or clusters)</li>
</ul></li>
<li>feature

<ul>
<li>the homogeneity (similarity) within the cluster is maximized</li>
<li>the heterogeneity (dissimilarity) between the cluster is maximized</li>
<li>unsupervised classification (no labels)</li>
<li>no predefined target class</li>
<li>number of clusters unknown</li>
<li>meaning of clusters unknown</li>
</ul></li>

<li><p>distance metrics</p>

<ul>
<li>measure the similarity and dissimilarity between observations in a group</li>

<li><p>types</p>

<ul>
<li><p>metrics for continuous variables</p>

<ul>
<li><p>Minkowski distance</p>

<ul>
<li>formula

<ul>
<li><span  class="math">\(n\)</span> represents the number of variables</li>
<li><span  class="math">\(x_i, x_j\)</span> are two observations</li>
</ul></li>
</ul>

<p><span  class="math">\[D(x_i:x_j)={(\sum_{k=1}^n {|x_{ik}-x_{jk}|}^p)}^{1/p}\]</span></p>

<ul>
<li>when p = 1, it is Manhattan (or city block) distance
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030602.png" alt=""></figure></li>
</ul>

<p><span  class="math">\[D(x_i:x_j)=|x_{11}-x_{21}|+|x_{12}-x_{22}|=|50-30|+|20-10|=30\]</span></p>

<ul>
<li>when p = 2, it is Euclidean distance</li>
</ul>

<p><span  class="math">\[D(x_i:x_j)=\sqrt{{(x_{11}-x_{21})}^2+{(x_{12}-x_{22})}^2}=\sqrt{{(50-30)}^2+{(20-10)}^2}=22\]</span></p></li>

<li><p>Pearson correlation</p></li>
</ul></li>

<li><p>metrics for categorical variables</p>

<ul>
<li>simple matching coefficient (SMC)

<ul>
<li>calculates the number of identical matches between the variable values</li>
<li>assumption of SMC is that &quot;Yes&quot; and &quot;No&quot; are of equal weights</li>
</ul></li>
<li>Jaccard index

<ul>
<li>similar to SMC, but left out the &quot;No-No&quot; match</li>
<li>measures the similarity between observations across those red flags that were raised at least once</li>
<li>especially useful in situations where many red-flag indicators are available and typically only a few are raised</li>
</ul></li>
<li>example

<ul>
<li>SMC(Claim1, Claim2) = 2/5</li>
<li>Jaccard(Claim1, Claim2) = 1/4
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030604.png" alt=""></figure></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<p><strong>Types of clustering</strong></p>

<ul>
<li><p>hierarchical</p>

<ul>
<li>agglomerative

<ul>
<li>bottom-up</li>
</ul></li>
<li>divisive

<ul>
<li>top-down
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030605.png" alt=""></figure><br></li>
</ul></li>
<li>use a similarity or distance matrix to merge or split one cluster at a time</li>

<li><p>distance matrics</p>

<ul>
<li>single linkage: smallest possible distance

<ul>
<li>thin, long and elongated clusters since dissimilar objects are not accounted for</li>
</ul></li>
<li>complete linkage: biggest possible distance

<ul>
<li>cluster are tight, more balanced and spherical</li>
</ul></li>
<li>average linkage: average of all possible distances

<ul>
<li>merge clusters with small variances, results in clusters with similar variance</li>
</ul></li>
<li>centroid method: distance between the centroids of both clusters

<ul>
<li>most robust to outliers, but not perform as well as Ward's method or average linkage
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030606.png" alt=""></figure></li>
</ul></li>

<li><p>Ward's distance: the dendrogram and hierarchy is clearer</p>

<ul>
<li>merge clusters with a small number of observations and results in balanced clusters</li>
<li>formula</li>
</ul>

<p><span  class="math">\[D_{Ward}(C_i,C_j)=\sum_{x\in C_i}{(x-c_i)}^2+\sum_{x\in C_j}{(x-c_j)}^2-\sum_{x\in C_{ij}}{(x-c_{ij})}^2\]</span></p>

<ul>
<li>find the optimal clustering thru

<ul>
<li>dendrogram - termination condition to cut the dendrogram</li>
<li>screen plot - elbow point indicates the optimal clustering
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030608.png" alt=""></figure></li>
</ul></li>
</ul></li>
</ul></li>

<li><p>advantages</p>

<ul>
<li>easy to interpret and understand (dendrogram is easy to understand)</li>
<li>relatively easy to implement</li>
<li>number of clusters does not need to be specified prior to the analysis</li>
</ul></li>

<li><p>disadvantages</p>

<ul>
<li>the methods do not scale very well to large data sets, i.e. computationally expansive</li>
<li>interpretation of clusters is often subjective and depends on expert knowledge</li>
<li>does not work well with mixed data types and missing data</li>
<li>not the best solution for data sets with high dimensions</li>
</ul></li>
</ul></li>

<li><p>non-hierarchical</p>

<ul>
<li>k-means

<ul>
<li>some basics

<ul>
<li>k means each cluster is represented by the center (or mean) of the cluster</li>
<li>need to be specified before the start of analysis</li>
</ul></li>
<li>procedure

<ul>
<li>select k observations as initial cluster centroids (seeds)</li>
<li>assign each observation to the cluster that has the closest centroid</li>
<li>when all observations have been assigned, recalculate the positions of the k centroids</li>
<li>repeat until the cluster centroids no longer change or a fixed number of iterations is reached
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030609.png" alt=""></figure></li>
</ul></li>
<li>advantages

<ul>
<li>relatively computational efficient as compared with hierarchical clustering</li>
<li>simple to implement and scales to large datasets</li>
</ul></li>
<li>disadvantages

<ul>
<li>need to specify k, the number of clusters, in advance</li>
<li>often terminates at a local optimum, need to try different initial cluster centres</li>
<li>unable to handle noisy data and outliers</li>
</ul></li>
</ul></li>
<li>self-organizing map (SOM)</li>
</ul></li>

<li><p>many others</p>

<ul>
<li>density-based</li>
<li>model-based</li>
<li>graph-based</li>
</ul></li>
</ul>

<h2 id="performance-evaluation-of-clustering">Performance Evaluation of Clustering</h2>

<p><strong>Overivew</strong></p>

<ul>
<li>method

<ul>
<li>no universal criterion for clustering performance evaluation</li>
</ul></li>
<li>only 2 perspectives

<ul>
<li>statistical perspective</li>
<li>interpretability perspective</li>
</ul></li>
<li>the standard of a good clustering solution

<ul>
<li>high within-cluster similarity</li>
<li>low between-cluster similarity</li>
</ul></li>
</ul>

<p><strong>SSE (Sum of Square Error)</strong></p>

<ul>
<li>a statistical method</li>
<li>how to evaluate

<ul>
<li>the lower the SSE for a particular cluster (WSS), the more homogenous is that cluster, i.e. higher within-cluster similarity</li>
<li>the higher the SSE among different clusters (BSS), the more heterogenous are the clusters, i.e. lower between-cluster similarity</li>
</ul></li>
<li>the optimal number of clusters

<ul>
<li>elbow method - makes use of within-cluster SSE (WSS)</li>
<li>steps

<ul>
<li>compute the k-means clustering models for different k values</li>
<li>for each k, calculate the WSS</li>
<li>plot WSS accoriding to the value of k</li>
<li>find the point where there is a bend (or elbow) in the curve -&gt; optimal
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21030901.png" alt=""></figure></li>
</ul></li>
</ul></li>
</ul>

<p><strong>Interpretation of a clustering solution</strong></p>

<ul>
<li>compare clusters with population distribution (or between clusters)

<ul>
<li>plot histogram, mean, bar charts etc of those most important variables by clusters</li>
<li>example

<ul>
<li>cluster C1 has observations with low recency values and high monetary values, whereas the frequency is similar to original population
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031101.png" alt=""></figure></li>
</ul></li>
</ul></li>
<li>use classification tree

<ul>
<li>to predict the target variables</li>
<li>example
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031102.png" alt=""></figure></li>
</ul></li>
</ul>

<h2 id="neural-network">Neural Network</h2>

<p><strong>Basic concepts</strong></p>

<ul>
<li>neural networks (NN)

<ul>
<li>can be used to perform tasks like classification, clustering, predictive analytics</li>
</ul></li>
<li>ANN &amp; DNN

<ul>
<li>artificial neural networks (ANN) are composed of layers of node/neurons</li>
<li>neural networks with more than one hidden layer are called deep neural networks (DNN)</li>
</ul></li>
<li>structure

<ul>
<li>input layer - initial input data for the neural network (visible layer)</li>
<li>hidden layers - between input and output layers, where the computations are done and passed on to other nodes deeper in the neural network</li>
<li>output layer - output result of the given input (visible layer)
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031103.png" alt=""></figure></li>
</ul></li>
</ul>

<p><strong>Inside an artificial neuron</strong></p>

<ul>
<li>activation functions

<ul>
<li>mathematical equations that determine the output of a neural network</li>
<li>attached to each node and defines if the given node should be “activated”, based on whether each node’s input is relevant for the model’s prediction</li>
<li>normalize the output of each neuron to a range between 1 and 0 or between -1 and 1
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/%E6%88%AA%E5%B1%8F2021-03-11%20%E4%B8%8B%E5%8D%883.53.17.png" alt=""></figure></li>
<li>examples

<ul>
<li>linear activation functions

<ul>
<li>threshold (unit step or sign) function

<ul>
<li>only classify output as 0 or 1</li>
</ul></li>
<li>linear function

<ul>
<li>transform the output of the neural network into a linear function, unable to cater for non-linear data
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031104.png" alt=""></figure></li>
</ul></li>
</ul></li>
<li>non-linear activation functions (most neural networks use this for more complex mappings between inputs and outputs)

<ul>
<li>Sigmoid function

<ul>
<li>can accept any value and normalize output between 0 to 1</li>
<li>smooth gradient and clear predictions</li>
<li>problems

<ul>
<li>output not zero centered &amp; computationally expensive</li>
<li>vanishing gradient - when the sigmoid function value is either too high or too low, the derivative becomes very small i.e. &lt;&lt; 1, causing poor learning for deep networks</li>
</ul></li>
</ul></li>
<li>TanH (hyperbolic tangent) function

<ul>
<li>similar to Sigmoid function, but zero centered, i.e. easier to model inputs with strong negative, neutral and positive values
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031105.png" alt=""></figure></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<p><strong>Training a neural network</strong></p>

<ul>
<li><p>cost/loss function</p>

<ul>
<li>NN is trained based on this</li>
<li>to measure the error of the network's prediction, or how good the neural network model is for a particular task</li>

<li><p>cost function - MSE</p>

<ul>
<li>difference between predicted output and actual output</li>
<li>square the difference and</li>
<li>calculate the mean</li>
</ul>

<p><span  class="math">\[MSE=\frac{1}{n}\sum_{i=1}^n{(Y_i-\hat{Y_i})}^2\]</span></p></li>

<li><p>MSE should be as small number as possible</p></li>
</ul></li>

<li><p>process
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031106.png" alt=""></figure></p></li>

<li><p>for fraud analytics</p>

<ul>
<li>usually one hidden layer is enough</li>
<li>theoretically can use different activation function at each node, but usually this is fixed for each layer</li>
<li>for hidden layers (black-box approach)

<ul>
<li>use non-linear functions, e.g. TanH function</li>
</ul></li>
<li>for output layers

<ul>
<li>classification targets (fraud vs non-fraud): use sigmoid function</li>
<li>regression target (e.g. amount of fraud): can use any function</li>
</ul></li>
<li>data pre-processing

<ul>
<li>normalize the continuous variables, e.g. using z-scores</li>
<li>reduce the number of categories for categorical variables</li>
</ul></li>
</ul></li>
</ul>

<p><strong>Interpret neural network</strong></p>

<ul>
<li>variable selection

<ul>
<li>features

<ul>
<li>to select those variables that actively contribute to the neural network output</li>
<li>does not give insight about the internal workings of the neural network</li>
</ul></li>
<li>Hinton diagram

<ul>
<li>visualizes the weights between inputs and hidden neurons as squares</li>
<li>size of square is proportional to size of the weight</li>
<li>color of the square represents sign of the weight (e.g. black = -ve, white = +ve)</li>
<li>e.g. “Income” variable can be removed
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031202.png" alt=""></figure></li>
</ul></li>
<li>variable selection procedures

<ul>
<li>inspect the Hinton diagram and remove the variable whose weights are closest to ZERO</li>
<li>re-estimate the neural network with the variable removed (to speed up the convergence, it could be beneficial to start from the previous weights)</li>
<li>continue with step 1 until a stopping criterion is met, the stopping criterion could be a decrease of predictive performance or a fixed number of steps</li>
</ul></li>
<li>backward variable selection procedures

<ul>
<li>build a neural network with all N variables</li>
<li>remove each variable in turn and re-estimate the network (this will give N networks each having N-1 variables)</li>
<li>remove the variable whose absence gives the best performing network (e.g. in terms of misclassification error, mean squared error)</li>
<li>repeat this procedure until the performance decreases significantly
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031203.png" alt=""></figure></li>
</ul></li>
</ul></li>
<li>rule extraction

<ul>
<li>feature

<ul>
<li>extract if-then classification rules</li>
</ul></li>
<li>decompositional approach

<ul>
<li>decompose the network’s internal workings by inspecting weights and/or activation values</li>
<li>example
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031204.png" alt=""></figure></li>
</ul></li>
<li>pedagogical approach

<ul>
<li>example
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031205.png" alt=""></figure></li>
</ul></li>
<li>performance evaluation

<ul>
<li>rules extraction sets evaluated in terms of

<ul>
<li>accuracy</li>
<li>conciseness (e.g. number of rules, number of conditions per rule)</li>
<li>fidelity – measures to what extent the extracted rule set succeeds in mimicking the neural network
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031206.png" alt=""></figure></li>
</ul></li>
<li>benchmark the extracted rules/trees with a tree built directly on the original data to see the benefit of going through the neural network</li>
</ul></li>
</ul></li>
<li>two-stage models

<ul>
<li>combines 2 models with both interpretability and performance</li>
<li>procedure

<ul>
<li>stage 1 (model interpretability)

<ul>
<li>to estimate an easy-to-understand model (e.g. linear regression, logistic regression)</li>
</ul></li>
<li>stage 2 (model performance)

<ul>
<li>to predict the errors made by the simple model using the same set of predictors</li>
</ul></li>
</ul></li>
<li>example
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031207.png" alt=""></figure></li>
</ul></li>
</ul>

<p><strong>Advantages and disadvantages</strong></p>

<ul>
<li>advantages

<ul>
<li>handle different types of input variables and support multivariate targets</li>
<li>support flexible and nonlinear relations between input and target variables</li>
<li>due to flexibility of the model, it is applicable to many problems</li>
</ul></li>
<li>disadvantages

<ul>
<li>black-box model, not easy to interpret</li>
<li>prone to overfit</li>
<li>requires handling of input attributes, e.g. data imputation
<br></li>
</ul></li>
</ul>

<h2 id="support-vector-machine-svm">Support Vector Machine (SVM)</h2>

<p><strong>Overview</strong></p>

<ul>
<li>issues with NN

<ul>
<li>the objective function is nonconvex (i.e. may have multiple local minima)</li>
<li>the effort needed to tune the number of hidden neurons</li>
</ul></li>
<li>basics of SVM

<ul>
<li>supervised learning model that can be used for both classification and regression tasks</li>
<li>more commonly used for classification tasks</li>
<li>basic idea - to find a line or a hyperplane that best divides the dataset into 2 classes</li>
</ul></li>
</ul>

<p><strong>Basic idea</strong></p>

<ul>
<li>linear classification

<ul>
<li>decision boundary

<ul>
<li>1-d: a dot</li>
<li>2-d: a line</li>
<li>n-d: hyperplane</li>
</ul></li>
<li>steps

<ul>
<li>find the points (support vector) that are closest to the hyperplane (the line) from both classes</li>
<li>compute the margin = distance between the support vectors and hyperplane</li>
<li>find the hyperplane with the maximal margins
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031208.png" alt=""></figure></li>
</ul></li>
<li>objective of SVM

<ul>
<li>to maximize margins between the two dotted lines</li>
<li>i.e. the 2 classes are far away as possible</li>
</ul></li>
<li>linear nonseparable

<ul>
<li>introduce some values

<ul>
<li>error term (<span  class="math">\(e_k\)</span>) allows for misclassification errors</li>
<li>hyperparameter (C) balances importance maximizing the margin or minimizing the error of on the data
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031209.png" alt=""></figure></li>
</ul></li>
<li>if it is linear separable, just ignore the above values</li>
</ul></li>
</ul></li>

<li><p>nonlinear classification</p>

<ul>
<li><p>mapping function <span  class="math">\(\phi(x)\)</span></p>

<ul>
<li>transform the input data points to a higher dimensional feature space</li>
<li>mathematically, it is not needed (need kernel function actually)</li>

<li><p>kernel trick</p>

<ul>
<li>formula
<br></li>
</ul>

<p><span  class="math">\[K(x_k,x_l)=\phi(x_k)^T\phi(x_l)\]</span></p>

<p><figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031210.png" alt=""></figure></p></li>
</ul></li>

<li><p>commonly used kernel functions</p>

<ul>
<li>linear kernel: <span  class="math">\(K(x,x_k)=x_k^Tx\)</span></li>
<li>polynomial kernel: <span  class="math">\(K(x,x_k)={(1+x_k^Tx)}^d\)</span></li>
<li>radial basis function (RBF) kernel: <span  class="math">\(K(x,x_k)=exp\{-{||x-x_k||}^2/\sigma^2\}\)</span></li>
<li>example

<ul>
<li>RBF kernel usually performs best, but you need to tune the extra parameter gamma <span  class="math">\(\sigma\)</span>
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031211.png" alt=""></figure></li>
</ul></li>

<li><p>tuning of hyperparameters</p>

<ul>
<li>hyperparameter C - penalty parameter representing an error or any form of misclassification

<ul>
<li>small C value

<ul>
<li>hyperplane with a larger margin, i.e. more violations are allowed, maximizing the margins</li>
</ul></li>
<li>large C value

<ul>
<li>hyperplane with a tighter margin, i.e. more emphasis is placed on minimizing the number of classifications
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031212.png" alt=""></figure></li>
</ul></li>
<li>just try to balance, which one should be used, thinking about the performance</li>
</ul></li>
<li>gamma

<ul>
<li>only need to tune this hyperparameter if you use nonlinear hyperplane</li>
<li>lower value of gamma

<ul>
<li>create a loose fit of the training dataset</li>
<li>consider only the nearby points for the calculation of a separate plane</li>
</ul></li>
<li>higher value of gamma

<ul>
<li>tries to exactly fit the training data</li>
<li>consider all the data-points to calculate the final separation line</li>
</ul></li>
<li>which one is better, defined by performance (each one is good, the difference is boundary)
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031213.png" alt=""></figure></li>
</ul></li>

<li><p>procedure</p>

<ul>
<li>partition the data into 40%, 30%, 30% training, validation and test data</li>

<li><p>build an RBF SVM classifier for each (<span  class="math">\(\sigma, C\)</span>)</p>

<p><span  class="math">\[\sigma \in \{0.5, 5, 10, 15, 25, 50, 100, 250, 500\}\]</span></p>

<p><span  class="math">\[C \in \{0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500\}\]</span></p></li>

<li><p>choose the (<span  class="math">\(\sigma, C\)</span>) combination with the best validation set performance</p></li>

<li><p>build an RBF SVM classifier with the optimal (<span  class="math">\(\sigma, C\)</span>) combination on combined training + validation dataset</p></li>

<li><p>calculate the performance of the estimated RBF SVM classifier on the test set</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>

<li><p>SVM interpretation</p>

<ul>
<li>SVM classifier can be presented as a neural network</li>
<li>then, use the techniques in neural network interpretation</li>
<li>SVM prediction -&gt; decision tree (like previous example)
<br></li>
</ul></li>
</ul>

<p><strong>Advantages and disadvantages</strong></p>

<ul>
<li>advantages

<ul>
<li>works really well with a clear margin of separation</li>
<li>effective in high dimensional spaces and where the number of dimensions is greater than the number of samples</li>
<li>uses a subset of training points in the decision function (called support vectors), so it is also memory efficient</li>
</ul></li>
<li>disadvantages

<ul>
<li>does not perform well when data set is large, and when the data set has more noise</li>
<li>choosing the effective kernel function can be computationally intensive</li>
</ul></li>
</ul>

<h2 id="multiclass-classification-techniques">Multiclass Classification Techniques</h2>

<p><strong>Binary vs multiclass</strong></p>

<ul>
<li>binary

<ul>
<li>label as &quot;fraud&quot; and &quot;no fraud&quot;</li>
</ul></li>
<li>multiclass

<ul>
<li>more than 2 labels</li>
<li>nominal labels

<ul>
<li>fraud, suspected fraud, no fraud</li>
</ul></li>
<li>ordinal labels

<ul>
<li>severe fraud, medium fraud, light fraud, no fraud
<br></li>
</ul></li>
</ul></li>
</ul>

<p><strong>Techniques</strong></p>

<ul>
<li><p>logistic regression (for nominal target variables)</p>

<ul>
<li><p>choose one of the target class (e.g. class K) as the base class</p>

<p><span  class="math">\[\frac{P(Y=1|X_1,\ldots,X_N)}{P(Y=K|X_1,\ldots,X_N)}=e^{(\beta_0^1+\beta_1^1X_1+\beta_2^2X_2+\ldots+\beta_N^1X_N)}\]</span></p>

<p><span  class="math">\[\frac{P(Y=2|X_1,\ldots,X_N)}{P(Y=K|X_1,\ldots,X_N)}=e^{(\beta_0^2+\beta_1^2X_1+\beta_2^2X_2+\ldots+\beta_N^2X_N)}\]</span></p>

<p><span  class="math">\[\cdots\]</span></p>

<p><span  class="math">\[\frac{P(Y=K-1|X_1,\ldots,X_N)}{P(Y=K|X_1,\ldots,X_N)}=e^{(\beta_0^{K-1}+\beta_1^{K-1}X_1+\beta_2^{K-1}X_2+\ldots+\beta_N^{K-1}X_N)}\]</span></p></li>

<li><p>based on the fact that all probabilities sum to 1</p>

<p><span  class="math">\[P(Y=1|X_1,\ldots,X_N)=\frac{e^{(\beta_0^1+\beta_1^1X_1+\beta_2^2X_2+\ldots+\beta_N^1X_N)}}{1+\sum_{k=1}^{K-1}e^{(\beta_0^k+\beta_1^kX_1+\beta_2^kX_2+\ldots+\beta_N^kX_N)}}\]</span></p>

<p><span  class="math">\[P(Y=2|X_1,\ldots,X_N)=\frac{e^{(\beta_0^2+\beta_1^2X_1+\beta_2^2X_2+\ldots+\beta_N^2X_N)}}{1+\sum_{k=1}^{K-1}e^{(\beta_0^k+\beta_1^kX_1+\beta_2^kX_2+\ldots+\beta_N^kX_N)}}\]</span></p>

<p><span  class="math">\[P(Y=K|X_1,\ldots,X_N)=\frac{1}{1+\sum_{k=1}^{K-1}e^{(\beta_0^k+\beta_1^kX_1+\beta_2^kX_2+\ldots+\beta_N^kX_N)}}\]</span></p></li>

<li><p>parameter <span  class="math">\(\beta\)</span> is estimated using maximum aposteriori estimation, which is extension of maximum likelihood estimation</p></li>
</ul></li>

<li><p>decision tree</p>

<ul>
<li><p>the impurity criteria can be written as follows (K is the number of class labels)</p>

<p><span  class="math">\[Entropy(S)=-\displaystyle \sum^K_{k=1}p_klog_2(p_k)\]</span></p>

<p><span  class="math">\[Gini(S)=\displaystyle \sum^K_{k=1}p_k(1-p_k)\]</span></p></li>

<li><p>the splitting and stopping decisions are the same as the binary class classifications</p></li>
</ul></li>

<li><p>NN</p>

<ul>
<li>increase the number of output neurons to K, where K corresponds to K class labels</li>
<li>an observation is assigned to the output neuron with the highest activation value
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031301.png" alt=""></figure></li>
</ul></li>

<li><p>SVM</p>

<ul>
<li>treat is as binary class problem by dividing the target classes into 2 classes</li>
<li>one-versus-one, for a K target classes, learn K SVMs as follows

<ul>
<li>classifies target as &quot;y=1&quot; vs &quot;y=2&quot;; &quot;y=1&quot; vs &quot;y=3&quot;; ... &quot;y=1&quot; vs &quot;y=K&quot;; &quot;y=2&quot; vs &quot;y=3&quot;; &quot;y=2&quot; vs &quot;y=4&quot;; ... &quot;y=2&quot; vs &quot;y=K&quot; ...</li>
<li>will have <span  class="math">\(\frac{K(K-1)}{2}\)</span> classifiers</li>
<li>use a (weighted) voting procedure for the final classification result
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031302.png" alt=""></figure></li>
</ul></li>
<li>one-versus-all, for a K target classes, learn K SVMs as follows

<ul>
<li>SVM#1, with target class label=1, classifies target as “y=1” vs “y≠1&quot;</li>
<li>SVM#2, with target class label=2, classifies target as “y=2” vs “y≠2&quot;</li>
<li>SVM#K, with target class label=K, classifies target as “y=K” vs “y≠K”</li>
<li>use the highest posterior probability to decide the classification result
<figure><img src="https://raw.githubusercontent.com/MarshalGao/image_hosting/master/hugo_images/21031303.png" alt=""></figure></li>
</ul></li>
</ul></li>
</ul>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://www.marshalgao.com/tags/fite7410/">fite7410</a></span>
        <span class="tag"><a href="https://www.marshalgao.com/tags/financial-fraud-analytics/">financial fraud analytics</a></span>
        <span class="tag"><a href="https://www.marshalgao.com/tags/fintech/">fintech</a></span>
        <span class="tag"><a href="https://www.marshalgao.com/tags/techniques/">techniques</a></span>
        
    </p>

      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="https://www.marshalgao.com/categories/hku/">HKU</a></span>
        
    </p>


      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        2352 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2021-03-06 11:46
        

         
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h"></span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://www.marshalgao.com/ecom7122-introduction-to-entrepreneurship-and-lean-startup/">
                <span class="button__icon">←</span>
                <span class="button__text">ECOM7122 Introduction to Entrepreneurship and Lean Startup</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="https://www.marshalgao.com/fite7410-financial-fraud-analytics-techniques-part-1/">
                <span class="button__text">FITE7410 Financial Fraud Analytics Techniques (Part 1)</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    


    

  </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            
            
          </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020-2021</span>
            
                
                <span>Marshal</a></span>
            
            
            
        </div>
    </div>
</footer>


            
        </div>

        




<script type="text/javascript" src="https://www.marshalgao.com/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>



    </body>
</html>
